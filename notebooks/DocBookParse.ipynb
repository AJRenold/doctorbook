{
 "metadata": {
  "name": "DocBookParse"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Dr. Book - Linking a Book to Open Data Sources</h2>\n",
      "<h3>Abstract:</h3>\n",
      "<p>For any text-based resource, we will attempt to extract named entities, such as people, places, topics and concepts, to render an annotated version of the text, linking the text to Open Data resources. Our tool parses books authored using the Docbook XML structure (http://docbook.org/tdg51/en/html/) and also uses natural language processing techniques to process raw text, attempting to extract terms which can be linked to wikipedia and other Open Data resources. Our tool is potentially useful for pre and post processing a text document. Pre-processing would take place during the authoring and publication process to create supplementatal resources for inclusion in the text. Post-processing would take place at reading time in the form of a browser or ereader extension which annotates the text of the page.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Original Idea:</h3>\n",
      "<p>Two of our team members are involved in the <i><a href=\"http://www.ischool.berkeley.edu/courses/i290-feb\">Future of eBooks</a></i> seminar in which students have created an <a href=\"www.futurepress.org\">open-source eBook reader</a>. We believe that books should be platform-independent and plugin-extensible.</p>\n",
      "<p>Our team decided to work on a project that would explore the use of Open Data to augment the reading experience by adding semantic web-like links to connect information within a book to the Web.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#imports\n",
      "from bs4 import BeautifulSoup, NavigableString, Tag\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import re\n",
      "from itertools import islice\n",
      "from collections import Counter\n",
      "import os\n",
      "\n",
      "# Our tools\n",
      "from get_wiki_links import WikiUrlFetch\n",
      "from get_wiki_text import Wiki2Plain"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>TDO: The Discipline of Organizing:</h3>\n",
      "<p>The team chose Prof. Glushko's upcoming textbook <i><a href=\"http://tdo.berkeley.edu/\">The Discipline of Organizing (TDO)</a></i> to use to develop ideas for our project. We thought the book would be a good test case for the project, as it is authored in DocBook XML, with structured markup of entities within the text, and many references to entities with wikipedia entries. This allowed us to work on XML and raw text parsing.</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Read Chapter1.xml into BeautifulSoup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PATH = 'Chapter1.xml'\n",
      "soup = BeautifulSoup(open(PATH, 'rt').read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Picking up tags:</h2>\n",
      "<p>TDO uses the <a href=\"http://docbook.org/\">DocBook</a> XML markup standard, providing a solid base for analyzing terms by parsing specific tags in the text.</p>\n",
      "<p>After exploring the text, we opted to use the tags \"keyword,\" \"author,\" \"indexterm,\" \"orgname,\" \"personname,\" and \"phrase.\"</p>\n",
      "<p>Which tags are in the soup?</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book_tags = set([tag.name for tag in soup.findAll(True)])\n",
      "print(book_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['ulink', 'surname', 'figure', 'footnote', 'othername', 'primary', 'uri', 'hardware', 'citerefentry', 'manvolnum', 'phrase', 'sidebar', 'tertiary', 'xref', 'sect2', 'para', 'sect1', 'orgname', 'highlights', 'sect1info', 'abbrev', 'filename', 'application', 'emphasis', 'html', 'listitem', 'textobject', 'indexterm', 'chapterinfo', 'firstterm', 'body', 'keywordset', 'blockquote', 'attribution', 'firstname', 'quote', 'symbol', 'literal', 'sect3info', 'citetitle', 'link', 'foreignphrase', 'secondary', 'chapter', 'mediaobject', 'sidebarinfo', 'imagedata', 'keyword', 'glossterm', 'author', 'imageobject', 'personname', 'sect2info', 'refentrytitle', 'action', 'title', 'itemizedlist', 'sect3'])\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using selected tags to construct a Pandas DataFrame with terms from the text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The following script looks for terms within select tags and creates a Pandas DataFrame object.\n",
      "# The DataFrame consists of \"count\" (term counter), \"tag\" (the XML tag for which the term is an attribute),\n",
      "# and \"term\" (the term found within the tag).\n",
      "\n",
      "list_for_df = []\n",
      "tags = ['keyword','author','indexterm','orgname','personname','phrase']\n",
      "for tag in tags:\n",
      "    for i in soup.findAll(tag):\n",
      "        if i.string == None:\n",
      "            list_for_df.append({ 'term':\" \".join([ child.lower().encode('utf-8') for child in i.stripped_strings ]), \\\n",
      "                                'tag': tag, 'count': 1 })\n",
      "        else:\n",
      "            list_for_df.append({ 'term': re.sub('  +','', i.string.lower().encode('utf-8')), 'tag': tag, 'count': 1 })\n",
      "        \n",
      "book_df = pd.DataFrame(list_for_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First 10 entries of the DataFrame.\n",
      "book_df[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>count</th>\n",
        "      <th>tag</th>\n",
        "      <th>term</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> keyword</td>\n",
        "      <td>                organize</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> keyword</td>\n",
        "      <td>       organizing system</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1</td>\n",
        "      <td> keyword</td>\n",
        "      <td>              discipline</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1</td>\n",
        "      <td> keyword</td>\n",
        "      <td>               framework</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 1</td>\n",
        "      <td> keyword</td>\n",
        "      <td>                resource</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> 1</td>\n",
        "      <td> keyword</td>\n",
        "      <td>              collection</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td> 1</td>\n",
        "      <td> keyword</td>\n",
        "      <td> intentional arrangement</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td> 1</td>\n",
        "      <td> keyword</td>\n",
        "      <td>                   agent</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td> 1</td>\n",
        "      <td> keyword</td>\n",
        "      <td>    description resource</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> 1</td>\n",
        "      <td> keyword</td>\n",
        "      <td>        primary resource</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "   count      tag                     term\n",
        "0      1  keyword                 organize\n",
        "1      1  keyword        organizing system\n",
        "2      1  keyword               discipline\n",
        "3      1  keyword                framework\n",
        "4      1  keyword                 resource\n",
        "5      1  keyword               collection\n",
        "6      1  keyword  intentional arrangement\n",
        "7      1  keyword                    agent\n",
        "8      1  keyword     description resource\n",
        "9      1  keyword         primary resource"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Text Processing with nltk to find Named Entities in the Text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Regex v. Natural Language Processing:</h3>\n",
      "<p>Our team explored both Regex-based, and Natural Language-based entity recognition techniques. We initially worked with Regex, thinking it would provide a more flexible, rule-based method of identifying terms with less overhead. We quickly realized that Natural Language Processing offers a better solution to our problem. As the old hacker adage goes: don't reinvent the wheel!</p>\n",
      "<p>We used <a href=\"http://nltk.org/\">NLTK</a>, a Python module rich in natural language tools, to tag words by part of speech. We then focused on parts of speech which could be mapped to named entities, nouns and proper nouns.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Here's the script for cleaning up the text and tagging each word.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We used NLTK (Natural Language Toolkit) to identify terms within the text that are not markedup\n",
      "# with XML.\n",
      "\n",
      "# First we create one large text by stripping the markup from XML.\n",
      "all_xml =  \" \".join([ line for line in soup.stripped_strings])\n",
      "text = nltk.clean_html(all_xml)\n",
      "\n",
      "# We then clean the text through string subsitutions.\n",
      "text = re.sub(r\"[\\n\\.,\\(\\)\\?\\!\\-':]\",\"\",text)\n",
      "\n",
      "# Finally, we use nltk.pos_tag to insert part-of-speech tags to each word in the text.\n",
      "tagged_text = nltk.pos_tag(text.split(' '))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Noun Chains</h3>\n",
      "<p>To create a list of potentially named entities in the document, we looked for nouns and variations of nouns (singlar v. plural, general nouns v. proper nouns). We then developed an algorithm to create chains of nouns that appear next to each other. The hypothesis for this is that two or more nouns appearing next to each other likely name the same entity, such as Berkeley California, Nunberg Geoff, or library information systems</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos = ['NN','NNS','NNP','NNPS'] ## nltk noun tags\n",
      "\n",
      "terms = []\n",
      "chain = []\n",
      "for term in tagged_text: ## loop through tagged text (in its original order)\n",
      "    \n",
      "    if term[1] in pos: ## if a term is in our pos list, append the term to the chain and to the noun list\n",
      "        chain.append(term)\n",
      "        terms.append(term[0].lower().encode('UTF-8'))\n",
      "        terms.append(\" \".join([ item[0].lower().encode('UTF-8') for item in chain ]))\n",
      "        \n",
      "    else: ## at the end of a sequence of nouns, append the chain to the noun list and reset the chain\n",
      "        if len(chain) > 0:\n",
      "            terms.append(\" \".join([ item[0].lower().encode('UTF-8') for item in chain ]))\n",
      "            chain = []\n",
      "            \n",
      "terms = [ re.sub(' +',' ',term) for term in terms if term != '' ] # clean extra spaces from noun chains\n",
      "terms = [ re.sub('^ ','',term) for term in terms ] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## sample of terms scrapped\n",
      "print terms[100:130]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['order', 'order', 'order', '', '', '', 'structure', 'structure', 'organizing', 'structure organizing', 'structure organizing', 'activity', 'activity', 'activity', 'shoes', 'shoes', 'shoes', 'closet', 'closet', 'closet', '', 'books', 'books', 'books', 'book', 'book', 'shelves', 'book shelves', 'book shelves', 'spices']\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Append new terms to DataFrame</h3>\n",
      "<p>As an approach to processing named entities from a large document, we choose to filter terms that occurred less than a certain number of times. We settled for 2 since setting it higher would have excluded many terms crucial to the understanding of a text. The issue here is that this method, while retaining precision, had very high recall.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "terms_for_df = []\n",
      "term_count = Counter(terms)\n",
      "\n",
      "# count_threshold should be a number that returns a good amount of information within a reasonable amount of time.\n",
      "# This depends on what the reader is looking for, and how much computational power they have.\n",
      "# Theoretically speaking, it could be 0 if a reader wants to see all terms, and can process that amount of data on their computer.\n",
      "count_threshold = 2\n",
      "\n",
      "for term,count in term_count.iteritems():\n",
      "    if count >= count_threshold:\n",
      "        terms_for_df.append({ 'term': term,'count': count })\n",
      "        #print term, count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# check state of original book_df (366 terms)\n",
      "book_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 366 entries, 0 to 365\n",
        "Data columns:\n",
        "count    366  non-null values\n",
        "tag      366  non-null values\n",
        "term     366  non-null values\n",
        "dtypes: int64(1), object(2)"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## create list of dict for appending to df\n",
      "list_for_df = []\n",
      "for item in terms_for_df:\n",
      "    if item['term'] != \"\" and item['term'] != \" \":\n",
      "        list_for_df.append({ 'term': item['term'], 'count': item['count'], 'tag': 'raw'})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Append terms scrapped from text to book_df"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book_df = book_df.append(pd.DataFrame(list_for_df))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## check the raw terms that were appended\n",
      "df_rawterms = book_df[book_df['tag']=='raw']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_rawterms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 2863 entries, 0 to 2862\n",
        "Data columns:\n",
        "count    2863  non-null values\n",
        "tag      2863  non-null values\n",
        "term     2863  non-null values\n",
        "dtypes: int64(1), object(2)"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# check state of new book_df (3229 terms)\n",
      "book_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 3229 entries, 0 to 2862\n",
        "Data columns:\n",
        "count    3229  non-null values\n",
        "tag      3229  non-null values\n",
        "term     3229  non-null values\n",
        "dtypes: int64(1), object(2)"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## drop duplicated terms\n",
      "book_df.drop_duplicates()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 3190 entries, 0 to 2862\n",
        "Data columns:\n",
        "count    3190  non-null values\n",
        "tag      3190  non-null values\n",
        "term     3190  non-null values\n",
        "dtypes: int64(1), object(2)"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Connecting to the web with DBPedia and Wikipedia:</h2>\n",
      "<p>Our team decided to connect all identified named entities to Wikipedia using a DBpedia Look-up API and DBPedia entity pages. We used an edit distance method to judge the quality of matched entities when the match was not exact. Matches are rated as being \"exact\" (exact match), \"good-partial\" (a likely useful partial match), \"partial\" (dubious match) and \"none\". The match ratings are stored in the DataFrame. After completing the code for this tool, we encapulated the tool in the class WikiUrlFetch, so that it could be easily reusable. This class is available in the appendix of this notebook and on github</p>\n",
      "\n",
      "<p>This method for matching text terms to Wikipedia entries created two of the main challenges of our project</p>\n",
      "<ol>\n",
      "    <li><b>Time dependence on 3rd party APIs</b> - as we worked on mapping named entities to wikipedia urls and expanded our tool from searching for a few terms to searching for thousands of terms, we realized that the response time of the DBPedia APIs was hindering the performance of our tool. For example matching the 3,000+ terms in a single chapter took upwards of three hours.</li>\n",
      "    <li><b>Evaluating match quality</b> - it is a challenge to judge the semantic quality of matches, therefore even good-partial matches do not aways have the same semantic meaning as the term searched</li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from get_wiki_links import WikiUrlFetch"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## example query with WikiUrlFetch\n",
      "w = WikiUrlFetch(\"Nunberg, Geoff\")\n",
      "w.results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "[{'match': 'good-partial',\n",
        "  'term': 'geoffrey nunberg',\n",
        "  'url': 'http://dbpedia.org/resource/Geoffrey_Nunberg',\n",
        "  'wiki_url': 'http://en.wikipedia.org/wiki/Geoffrey_Nunberg'}]"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Using WikiUrlFetch, look up wikipedia URLs for terms in the text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import islice\n",
      "import time\n",
      "\n",
      "new_cols = []\n",
      "\n",
      "for row in islice(book_df.iterrows(),100): # iterations limited 100 because WikiUrlFetch because of API limitations\n",
      "\n",
      "    r = WikiUrlFetch(row[1]['term'])\n",
      "    wikis = r.results\n",
      "    \n",
      "    for wiki in wikis:\n",
      "        if wiki['match'] != 'none':\n",
      "            new_cols.append( { 'term': row[1]['term'], 'matched_term': wiki['term'], \\\n",
      "              'count': row[1]['count'], 'match': wiki['match'], 'url': wiki['wiki_url'] } )\n",
      "        else:\n",
      "            new_cols.append( { 'term': row[1]['term'], 'matched_term': wiki['term'], \\\n",
      "              'count': row[1]['count'], 'match': wiki['match'], 'url': \"\" } )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[7, {'url': 'http://dbpedia.org/resource/Labor_union', 'term': 'labor union'}], [7, {'url': 'http://dbpedia.org/resource/Union_organizer', 'term': 'union organizer'}], [10, {'url': 'http://dbpedia.org/resource/Labor_rights', 'term': 'labor rights'}], [10, {'url': 'http://dbpedia.org/resource/Self-organization', 'term': 'self-organization'}], [13, {'url': 'http://dbpedia.org/resource/Community_organizing', 'term': 'community organizing'}], [15, {'url': 'http://dbpedia.org/resource/Project_management', 'term': 'project management'}], [18, {'url': 'http://dbpedia.org/resource/Freedom_of_assembly', 'term': 'freedom of assembly'}], [27, {'url': 'http://dbpedia.org/resource/Territories_of_the_United_States', 'term': 'territories of the united states'}]]\n",
        "http://en.wikipedia.org/wiki/Labor_union"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Project_management"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[14, {'url': 'http://dbpedia.org/resource/Collage', 'term': 'collage'}], [14, {'url': 'http://dbpedia.org/resource/Grassroots', 'term': 'grassroots'}], [15, {'url': 'http://dbpedia.org/resource/Labor_union', 'term': 'labor union'}], [16, {'url': 'http://dbpedia.org/resource/1994_Winter_Olympics', 'term': '1994 winter olympics'}], [16, {'url': 'http://dbpedia.org/resource/Political_campaign', 'term': 'political campaign'}], [16, {'url': 'http://dbpedia.org/resource/Secretary', 'term': 'secretary'}], [25, {'url': 'http://dbpedia.org/resource/Territories_of_the_United_States', 'term': 'territories of the united states'}], [28, {'url': 'http://dbpedia.org/resource/International_Paralympic_Committee', 'term': 'international paralympic committee'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Secretary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/1994_Winter_Olympics"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[9, {'url': 'http://dbpedia.org/resource/BDSM', 'term': 'bdsm'}], [9, {'url': 'http://dbpedia.org/resource/Monasticism', 'term': 'monasticism'}], [10, {'url': 'http://dbpedia.org/resource/Prefect', 'term': 'prefect'}], [10, {'url': 'http://dbpedia.org/resource/Taboo', 'term': 'taboo'}], [10, {'url': 'http://dbpedia.org/resource/Tridentine_Mass', 'term': 'tridentine mass'}], [12, {'url': 'http://dbpedia.org/resource/Specialist_school', 'term': 'specialist school'}], [16, {'url': 'http://dbpedia.org/resource/Corporal_punishment', 'term': 'corporal punishment'}], [16, {'url': 'http://dbpedia.org/resource/Information_systems', 'term': 'information systems'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Taboo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Monasticism"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[5, {'url': 'http://dbpedia.org/resource/.NET_Framework', 'term': '.net framework'}], [8, {'url': 'http://dbpedia.org/resource/Morality', 'term': 'morality'}], [8, {'url': 'http://dbpedia.org/resource/Truss', 'term': 'truss'}], [11, {'url': 'http://dbpedia.org/resource/Legislation', 'term': 'legislation'}], [12, {'url': 'http://dbpedia.org/resource/Constitution', 'term': 'constitution'}], [15, {'url': 'http://dbpedia.org/resource/European_Union_law', 'term': 'european union law'}], [15, {'url': 'http://dbpedia.org/resource/Quantum_field_theory', 'term': 'quantum field theory'}], [20, {'url': 'http://dbpedia.org/resource/Software_development_kit', 'term': 'software development kit'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Constitution"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Morality"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[6, {'url': 'http://dbpedia.org/resource/Fishery', 'term': 'fishery'}], [8, {'url': 'http://dbpedia.org/resource/Wind', 'term': 'wind'}], [9, {'url': 'http://dbpedia.org/resource/Protected_area', 'term': 'protected area'}], [12, {'url': 'http://dbpedia.org/resource/Public_domain', 'term': 'public domain'}], [14, {'url': 'http://dbpedia.org/resource/National_security', 'term': 'national security'}], [16, {'url': 'http://dbpedia.org/resource/Uniform_resource_locator', 'term': 'uniform resource locator'}], [18, {'url': 'http://dbpedia.org/resource/Conservation_movement', 'term': 'conservation movement'}], [18, {'url': 'http://dbpedia.org/resource/Contributing_property', 'term': 'contributing property'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Public_domain"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[8, {'url': 'http://dbpedia.org/resource/Toll_road', 'term': 'toll road'}], [9, {'url': 'http://dbpedia.org/resource/Arboretum', 'term': 'arboretum'}], [9, {'url': 'http://dbpedia.org/resource/Concubinage', 'term': 'concubinage'}], [9, {'url': 'http://dbpedia.org/resource/Tate', 'term': 'tate'}], [10, {'url': 'http://dbpedia.org/resource/Anthology', 'term': 'anthology'}], [11, {'url': 'http://dbpedia.org/resource/Type_(biology)', 'term': 'type (biology)'}], [16, {'url': 'http://dbpedia.org/resource/Greatest_hits_album', 'term': 'greatest hits album'}], [19, {'url': 'http://dbpedia.org/resource/Trade_paperback_(comics)', 'term': 'trade paperback (comics)'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Greatest_hits_album"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Toll_road"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[10, {'url': 'http://dbpedia.org/resource/Intentional_community', 'term': 'intentional community'}], [12, {'url': 'http://dbpedia.org/resource/International_community', 'term': 'international community'}], [19, {'url': 'http://dbpedia.org/resource/Electromagnetic_interference', 'term': 'electromagnetic interference'}], [20, {'url': 'http://dbpedia.org/resource/Assault', 'term': 'assault'}], [20, {'url': 'http://dbpedia.org/resource/Commune', 'term': 'commune'}], [20, {'url': 'http://dbpedia.org/resource/Homicide', 'term': 'homicide'}], [20, {'url': 'http://dbpedia.org/resource/Teleology', 'term': 'teleology'}], [22, {'url': 'http://dbpedia.org/resource/Self-harm', 'term': 'self-harm'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Assault"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Homicide"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[5, {'url': 'http://dbpedia.org/resource/Free_agent', 'term': 'free agent'}], [5, {'url': 'http://dbpedia.org/resource/Pathogen', 'term': 'pathogen'}], [5, {'url': 'http://dbpedia.org/resource/Redox', 'term': 'redox'}], [8, {'url': 'http://dbpedia.org/resource/Espionage', 'term': 'espionage'}], [10, {'url': 'http://dbpedia.org/resource/Chemotherapy', 'term': 'chemotherapy'}], [10, {'url': 'http://dbpedia.org/resource/Inflammation', 'term': 'inflammation'}], [19, {'url': 'http://dbpedia.org/resource/Undrafted_sportsperson', 'term': 'undrafted sportsperson'}], [27, {'url': 'http://dbpedia.org/resource/Federal_Bureau_of_Investigation', 'term': 'federal bureau of investigation'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Federal_Bureau_of_Investigation"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Free_agent"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[15, {'url': 'http://dbpedia.org/resource/Generic_trademark', 'term': 'generic trademark'}], [16, {'url': 'http://dbpedia.org/resource/Ethnology', 'term': 'ethnology'}], [17, {'url': 'http://dbpedia.org/resource/Alpha_taxonomy', 'term': 'alpha taxonomy'}], [17, {'url': 'http://dbpedia.org/resource/Binomial_nomenclature', 'term': 'binomial nomenclature'}], [17, {'url': 'http://dbpedia.org/resource/Type_(biology)', 'term': 'type (biology)'}], [18, {'url': 'http://dbpedia.org/resource/Quantum_field_theory', 'term': 'quantum field theory'}], [20, {'url': 'http://dbpedia.org/resource/Quantum_electrodynamics', 'term': 'quantum electrodynamics'}], [27, {'url': \"http://dbpedia.org/resource/Newton's_law_of_universal_gravitation\", 'term': \"newton's law of universal gravitation\"}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Type_%28biology%29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Binomial_nomenclature"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[7, {'url': 'http://dbpedia.org/resource/Primary_school', 'term': 'primary school'}], [10, {'url': 'http://dbpedia.org/resource/Private_school', 'term': 'private school'}], [12, {'url': 'http://dbpedia.org/resource/Herbivore', 'term': 'herbivore'}], [13, {'url': 'http://dbpedia.org/resource/Elementary_school', 'term': 'elementary school'}], [14, {'url': 'http://dbpedia.org/resource/Penang', 'term': 'penang'}], [15, {'url': 'http://dbpedia.org/resource/Primary_election', 'term': 'primary election'}], [18, {'url': 'http://dbpedia.org/resource/Primary_sector_of_the_economy', 'term': 'primary sector of the economy'}], [24, {'url': 'http://dbpedia.org/resource/Metropolitan_Statistical_Area', 'term': 'metropolitan statistical area'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Primary_school"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Primary_sector_of_the_economy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[0, {'url': 'http://dbpedia.org/resource/Metadata', 'term': 'metadata'}], [6, {'url': 'http://dbpedia.org/resource/Tag_(metadata)', 'term': 'tag (metadata)'}], [7, {'url': 'http://dbpedia.org/resource/Meta_element', 'term': 'meta element'}], [8, {'url': 'http://dbpedia.org/resource/Identifier', 'term': 'identifier'}], [10, {'url': 'http://dbpedia.org/resource/Data_element', 'term': 'data element'}], [10, {'url': 'http://dbpedia.org/resource/Dublin_Core', 'term': 'dublin core'}], [20, {'url': 'http://dbpedia.org/resource/Extensible_Metadata_Platform', 'term': 'extensible metadata platform'}], [26, {'url': 'http://dbpedia.org/resource/Exchangeable_image_file_format', 'term': 'exchangeable image file format'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Metadata"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[9, {'url': 'http://dbpedia.org/resource/Robert_Christgau', 'term': 'robert christgau'}], [9, {'url': 'http://dbpedia.org/resource/Robert_E._Lee', 'term': 'robert e. lee'}], [10, {'url': 'http://dbpedia.org/resource/Robert_F._Kennedy', 'term': 'robert f. kennedy'}], [10, {'url': 'http://dbpedia.org/resource/Robin_Hood', 'term': 'robin hood'}], [13, {'url': 'http://dbpedia.org/resource/Bob_Marley', 'term': 'bob marley'}], [14, {'url': 'http://dbpedia.org/resource/Charles_Darwin', 'term': 'charles darwin'}], [15, {'url': 'http://dbpedia.org/resource/Hulk_(comics)', 'term': 'hulk (comics)'}], [15, {'url': 'http://dbpedia.org/resource/Robert_Schumann', 'term': 'robert schumann'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Charles_Darwin"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Robert_E._Lee"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[14, {'url': 'http://dbpedia.org/resource/Grassroots', 'term': 'grassroots'}], [15, {'url': 'http://dbpedia.org/resource/Collage', 'term': 'collage'}], [16, {'url': 'http://dbpedia.org/resource/Labor_union', 'term': 'labor union'}], [16, {'url': 'http://dbpedia.org/resource/Political_campaign', 'term': 'political campaign'}], [17, {'url': 'http://dbpedia.org/resource/1994_Winter_Olympics', 'term': '1994 winter olympics'}], [17, {'url': 'http://dbpedia.org/resource/Secretary', 'term': 'secretary'}], [26, {'url': 'http://dbpedia.org/resource/Territories_of_the_United_States', 'term': 'territories of the united states'}], [28, {'url': 'http://dbpedia.org/resource/International_Paralympic_Committee', 'term': 'international paralympic committee'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Secretary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/1994_Winter_Olympics"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[14, {'url': 'http://dbpedia.org/resource/Collage', 'term': 'collage'}], [14, {'url': 'http://dbpedia.org/resource/Grassroots', 'term': 'grassroots'}], [15, {'url': 'http://dbpedia.org/resource/Labor_union', 'term': 'labor union'}], [16, {'url': 'http://dbpedia.org/resource/1994_Winter_Olympics', 'term': '1994 winter olympics'}], [16, {'url': 'http://dbpedia.org/resource/Political_campaign', 'term': 'political campaign'}], [16, {'url': 'http://dbpedia.org/resource/Secretary', 'term': 'secretary'}], [25, {'url': 'http://dbpedia.org/resource/Territories_of_the_United_States', 'term': 'territories of the united states'}], [28, {'url': 'http://dbpedia.org/resource/International_Paralympic_Committee', 'term': 'international paralympic committee'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Secretary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/1994_Winter_Olympics"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[15, {'url': 'http://dbpedia.org/resource/Concubinage', 'term': 'concubinage'}], [16, {'url': 'http://dbpedia.org/resource/Arboretum', 'term': 'arboretum'}], [16, {'url': 'http://dbpedia.org/resource/Greatest_hits_album', 'term': 'greatest hits album'}], [17, {'url': 'http://dbpedia.org/resource/Anthology', 'term': 'anthology'}], [17, {'url': 'http://dbpedia.org/resource/Toll_road', 'term': 'toll road'}], [17, {'url': 'http://dbpedia.org/resource/Type_(biology)', 'term': 'type (biology)'}], [18, {'url': 'http://dbpedia.org/resource/Tate', 'term': 'tate'}], [20, {'url': 'http://dbpedia.org/resource/Trade_paperback_(comics)', 'term': 'trade paperback (comics)'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Greatest_hits_album"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Toll_road"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[10, {'url': 'http://dbpedia.org/resource/Intentional_community', 'term': 'intentional community'}], [12, {'url': 'http://dbpedia.org/resource/International_community', 'term': 'international community'}], [19, {'url': 'http://dbpedia.org/resource/Electromagnetic_interference', 'term': 'electromagnetic interference'}], [20, {'url': 'http://dbpedia.org/resource/Assault', 'term': 'assault'}], [20, {'url': 'http://dbpedia.org/resource/Commune', 'term': 'commune'}], [20, {'url': 'http://dbpedia.org/resource/Homicide', 'term': 'homicide'}], [20, {'url': 'http://dbpedia.org/resource/Teleology', 'term': 'teleology'}], [22, {'url': 'http://dbpedia.org/resource/Self-harm', 'term': 'self-harm'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Assault"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Homicide"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[8, {'url': 'http://dbpedia.org/resource/Gravitation', 'term': 'gravitation'}], [8, {'url': 'http://dbpedia.org/resource/Pollination', 'term': 'pollination'}], [10, {'url': 'http://dbpedia.org/resource/Symbiosis', 'term': 'symbiosis'}], [11, {'url': 'http://dbpedia.org/resource/Chemical_bond', 'term': 'chemical bond'}], [11, {'url': 'http://dbpedia.org/resource/Norm_(social)', 'term': 'norm (social)'}], [13, {'url': 'http://dbpedia.org/resource/Electromagnetism', 'term': 'electromagnetism'}], [14, {'url': 'http://dbpedia.org/resource/Crystal_structure', 'term': 'crystal structure'}], [18, {'url': 'http://dbpedia.org/resource/Protein\\xe2\\x80\\x93protein_interaction', 'term': 'protein\\xe2\\x80\\x93protein interaction'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Gravitation"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[12, {'url': 'http://dbpedia.org/resource/National_security', 'term': 'national security'}], [14, {'url': 'http://dbpedia.org/resource/Fishery', 'term': 'fishery'}], [14, {'url': 'http://dbpedia.org/resource/Protected_area', 'term': 'protected area'}], [14, {'url': 'http://dbpedia.org/resource/Public_domain', 'term': 'public domain'}], [16, {'url': 'http://dbpedia.org/resource/Uniform_resource_locator', 'term': 'uniform resource locator'}], [16, {'url': 'http://dbpedia.org/resource/Wind', 'term': 'wind'}], [17, {'url': 'http://dbpedia.org/resource/Conservation_movement', 'term': 'conservation movement'}], [18, {'url': 'http://dbpedia.org/resource/Contributing_property', 'term': 'contributing property'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Public_domain"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[11, {'url': 'http://dbpedia.org/resource/National_security', 'term': 'national security'}], [13, {'url': 'http://dbpedia.org/resource/Fishery', 'term': 'fishery'}], [13, {'url': 'http://dbpedia.org/resource/Protected_area', 'term': 'protected area'}], [13, {'url': 'http://dbpedia.org/resource/Public_domain', 'term': 'public domain'}], [14, {'url': 'http://dbpedia.org/resource/Uniform_resource_locator', 'term': 'uniform resource locator'}], [15, {'url': 'http://dbpedia.org/resource/Wind', 'term': 'wind'}], [17, {'url': 'http://dbpedia.org/resource/Contributing_property', 'term': 'contributing property'}], [18, {'url': 'http://dbpedia.org/resource/Conservation_movement', 'term': 'conservation movement'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Public_domain"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[8, {'url': 'http://dbpedia.org/resource/Physics', 'term': 'physics'}], [8, {'url': 'http://dbpedia.org/resource/Physiology', 'term': 'physiology'}], [10, {'url': 'http://dbpedia.org/resource/ARIA_Charts', 'term': 'aria charts'}], [10, {'url': 'http://dbpedia.org/resource/Health', 'term': 'health'}], [12, {'url': 'http://dbpedia.org/resource/Compact_Disc', 'term': 'compact disc'}], [14, {'url': 'http://dbpedia.org/resource/Energy', 'term': 'energy'}], [15, {'url': 'http://dbpedia.org/resource/Physical_education', 'term': 'physical education'}], [18, {'url': 'http://dbpedia.org/resource/Random-access_memory', 'term': 'random-access memory'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Physics"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[11, {'url': 'http://dbpedia.org/resource/Billboard_charts', 'term': 'billboard charts'}], [12, {'url': 'http://dbpedia.org/resource/ITunes_Store', 'term': 'itunes store'}], [12, {'url': 'http://dbpedia.org/resource/Music_download', 'term': 'music download'}], [13, {'url': 'http://dbpedia.org/resource/Computer', 'term': 'computer'}], [13, {'url': 'http://dbpedia.org/resource/Sampling_(music)', 'term': 'sampling (music)'}], [14, {'url': 'http://dbpedia.org/resource/Extended_play', 'term': 'extended play'}], [14, {'url': 'http://dbpedia.org/resource/MP3', 'term': 'mp3'}], [19, {'url': 'http://dbpedia.org/resource/High-definition_television', 'term': 'high-definition television'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Extended_play"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Computer"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[7, {'url': 'http://dbpedia.org/resource/Labor_union', 'term': 'labor union'}], [7, {'url': 'http://dbpedia.org/resource/Union_organizer', 'term': 'union organizer'}], [10, {'url': 'http://dbpedia.org/resource/Labor_rights', 'term': 'labor rights'}], [10, {'url': 'http://dbpedia.org/resource/Self-organization', 'term': 'self-organization'}], [13, {'url': 'http://dbpedia.org/resource/Community_organizing', 'term': 'community organizing'}], [15, {'url': 'http://dbpedia.org/resource/Project_management', 'term': 'project management'}], [18, {'url': 'http://dbpedia.org/resource/Freedom_of_assembly', 'term': 'freedom of assembly'}], [27, {'url': 'http://dbpedia.org/resource/Territories_of_the_United_States', 'term': 'territories of the united states'}]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://en.wikipedia.org/wiki/Labor_union"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(new_cols)\n",
      "new_cols_df = pd.DataFrame(new_cols)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## extract the exact matches\n",
      "exact_match = new_cols_df[new_cols_df['match'] == 'exact']\n",
      "exact_match[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>match</th>\n",
        "      <th>matched_term</th>\n",
        "      <th>term</th>\n",
        "      <th>url</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>22 </th>\n",
        "      <td> exact</td>\n",
        "      <td>       metadata</td>\n",
        "      <td>        metadata</td>\n",
        "      <td>       http://en.wikipedia.org/wiki/Metadata</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87 </th>\n",
        "      <td> exact</td>\n",
        "      <td>     cataloging</td>\n",
        "      <td>      cataloging</td>\n",
        "      <td>     http://en.wikipedia.org/wiki/Cataloging</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>158</th>\n",
        "      <td> exact</td>\n",
        "      <td>     adam smith</td>\n",
        "      <td>     smith, adam</td>\n",
        "      <td>     http://en.wikipedia.org/wiki/Adam_Smith</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>159</th>\n",
        "      <td> exact</td>\n",
        "      <td> charles darwin</td>\n",
        "      <td> darwin, charles</td>\n",
        "      <td> http://en.wikipedia.org/wiki/Charles_Darwin</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>167</th>\n",
        "      <td> exact</td>\n",
        "      <td>           cern</td>\n",
        "      <td>            cern</td>\n",
        "      <td>           http://en.wikipedia.org/wiki/CERN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 95,
       "text": [
        "     match    matched_term             term  \\\n",
        "22   exact        metadata         metadata   \n",
        "87   exact      cataloging       cataloging   \n",
        "158  exact      adam smith      smith, adam   \n",
        "159  exact  charles darwin  darwin, charles   \n",
        "167  exact            cern             cern   \n",
        "\n",
        "                                             url  \n",
        "22         http://en.wikipedia.org/wiki/Metadata  \n",
        "87       http://en.wikipedia.org/wiki/Cataloging  \n",
        "158      http://en.wikipedia.org/wiki/Adam_Smith  \n",
        "159  http://en.wikipedia.org/wiki/Charles_Darwin  \n",
        "167            http://en.wikipedia.org/wiki/CERN  "
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## extract the good matches\n",
      "good_match = new_cols_df[new_cols_df['match'] == 'good-partial']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## extract the partial matches\n",
      "partial_match = new_cols_df[new_cols_df['match'] == 'partial']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Create a DataFrame with only good-partial and exact match terms\n",
      "matched_terms = good_match.append(exact_match)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## first 10 matched_terms\n",
      "matched_terms[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>match</th>\n",
        "      <th>matched_term</th>\n",
        "      <th>term</th>\n",
        "      <th>url</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>56 </th>\n",
        "      <td> good-partial</td>\n",
        "      <td> geoffrey nunberg</td>\n",
        "      <td>  nunberg, geoff</td>\n",
        "      <td> http://en.wikipedia.org/wiki/Geoffrey_Nunberg</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>106</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>              rai</td>\n",
        "      <td>             uri</td>\n",
        "      <td>              http://en.wikipedia.org/wiki/RAI</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>109</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>              rai</td>\n",
        "      <td>             uri</td>\n",
        "      <td>              http://en.wikipedia.org/wiki/RAI</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>142</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>         data set</td>\n",
        "      <td>         dataset</td>\n",
        "      <td>         http://en.wikipedia.org/wiki/Data_set</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22 </th>\n",
        "      <td>        exact</td>\n",
        "      <td>         metadata</td>\n",
        "      <td>        metadata</td>\n",
        "      <td>         http://en.wikipedia.org/wiki/Metadata</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87 </th>\n",
        "      <td>        exact</td>\n",
        "      <td>       cataloging</td>\n",
        "      <td>      cataloging</td>\n",
        "      <td>       http://en.wikipedia.org/wiki/Cataloging</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>158</th>\n",
        "      <td>        exact</td>\n",
        "      <td>       adam smith</td>\n",
        "      <td>     smith, adam</td>\n",
        "      <td>       http://en.wikipedia.org/wiki/Adam_Smith</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>159</th>\n",
        "      <td>        exact</td>\n",
        "      <td>   charles darwin</td>\n",
        "      <td> darwin, charles</td>\n",
        "      <td>   http://en.wikipedia.org/wiki/Charles_Darwin</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>167</th>\n",
        "      <td>        exact</td>\n",
        "      <td>             cern</td>\n",
        "      <td>            cern</td>\n",
        "      <td>             http://en.wikipedia.org/wiki/CERN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 109,
       "text": [
        "            match      matched_term             term  \\\n",
        "56   good-partial  geoffrey nunberg   nunberg, geoff   \n",
        "106  good-partial               rai              uri   \n",
        "109  good-partial               rai              uri   \n",
        "142  good-partial          data set          dataset   \n",
        "22          exact          metadata         metadata   \n",
        "87          exact        cataloging       cataloging   \n",
        "158         exact        adam smith      smith, adam   \n",
        "159         exact    charles darwin  darwin, charles   \n",
        "167         exact              cern             cern   \n",
        "\n",
        "                                               url  \n",
        "56   http://en.wikipedia.org/wiki/Geoffrey_Nunberg  \n",
        "106               http://en.wikipedia.org/wiki/RAI  \n",
        "109               http://en.wikipedia.org/wiki/RAI  \n",
        "142          http://en.wikipedia.org/wiki/Data_set  \n",
        "22           http://en.wikipedia.org/wiki/Metadata  \n",
        "87         http://en.wikipedia.org/wiki/Cataloging  \n",
        "158        http://en.wikipedia.org/wiki/Adam_Smith  \n",
        "159    http://en.wikipedia.org/wiki/Charles_Darwin  \n",
        "167              http://en.wikipedia.org/wiki/CERN  "
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "After we get the Wikipedia URL, we use Wiki2Plain to extract the page summary and image for the matched_terms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>We chose \"exact\" and \"good-partial\" matches for Wikipedia linking. Many of the terms returned correct results (see results below), while some returned incorrect pages, e.g. Radio Televisione Italiana for URI. This shows the challenge of finding good results when looking up Wikipedia URLs. The class Wiki2Plain, allows us to scrape the summary and image from a Wikipedia page. The class is available in the Appendix and on Github.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## function using Wiki2Plain results to create two new DataFrame columns using apply\n",
      "def get_wiki_textimage(url):\n",
      "    wiki = Wiki2Plain(url)\n",
      "    return wiki.text, wiki.image()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## apply and use tuple result to create two DataFrames\n",
      "## from http://manishamde.github.io/blog/2013/03/07/pandas-and-python-top-10/#create\n",
      "\n",
      "matched_terms['wiki_text'],matched_terms['wiki_image'] = zip(*matched_terms['url'].apply(get_wiki_textimage))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## first ten terms\n",
      "matched_terms[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>match</th>\n",
        "      <th>matched_term</th>\n",
        "      <th>term</th>\n",
        "      <th>url</th>\n",
        "      <th>wiki_text</th>\n",
        "      <th>wiki_image</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>56 </th>\n",
        "      <td> good-partial</td>\n",
        "      <td> geoffrey nunberg</td>\n",
        "      <td>  nunberg, geoff</td>\n",
        "      <td> http://en.wikipedia.org/wiki/Geoffrey_Nunberg</td>\n",
        "      <td> Geoffrey Nunberg (born June, 1945) is an Ameri...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>106</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>              rai</td>\n",
        "      <td>             uri</td>\n",
        "      <td>              http://en.wikipedia.org/wiki/RAI</td>\n",
        "      <td> RAI &mdash; Radiotelevisione Italiana S.p.A. (...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>109</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>              rai</td>\n",
        "      <td>             uri</td>\n",
        "      <td>              http://en.wikipedia.org/wiki/RAI</td>\n",
        "      <td> RAI &mdash; Radiotelevisione Italiana S.p.A. (...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>142</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>         data set</td>\n",
        "      <td>         dataset</td>\n",
        "      <td>         http://en.wikipedia.org/wiki/Data_set</td>\n",
        "      <td> A dataset (or data set) is a collection of dat...</td>\n",
        "      <td>                                              None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22 </th>\n",
        "      <td>        exact</td>\n",
        "      <td>         metadata</td>\n",
        "      <td>        metadata</td>\n",
        "      <td>         http://en.wikipedia.org/wiki/Metadata</td>\n",
        "      <td> The term metadata refers to \"data about data\"....</td>\n",
        "      <td>                                              None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87 </th>\n",
        "      <td>        exact</td>\n",
        "      <td>       cataloging</td>\n",
        "      <td>      cataloging</td>\n",
        "      <td>       http://en.wikipedia.org/wiki/Cataloging</td>\n",
        "      <td> Cataloging (or cataloguing) is the process of ...</td>\n",
        "      <td>                                              None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>158</th>\n",
        "      <td>        exact</td>\n",
        "      <td>       adam smith</td>\n",
        "      <td>     smith, adam</td>\n",
        "      <td>       http://en.wikipedia.org/wiki/Adam_Smith</td>\n",
        "      <td> Adam Smith (5 June 1723 OS \u2013 17 July 1790) was...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>159</th>\n",
        "      <td>        exact</td>\n",
        "      <td>   charles darwin</td>\n",
        "      <td> darwin, charles</td>\n",
        "      <td>   http://en.wikipedia.org/wiki/Charles_Darwin</td>\n",
        "      <td> Charles Robert Darwin, FRS (12 February 1809 \u2013...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>167</th>\n",
        "      <td>        exact</td>\n",
        "      <td>             cern</td>\n",
        "      <td>            cern</td>\n",
        "      <td>             http://en.wikipedia.org/wiki/CERN</td>\n",
        "      <td> The European Organization for Nuclear Research...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "            match      matched_term             term  \\\n",
        "56   good-partial  geoffrey nunberg   nunberg, geoff   \n",
        "106  good-partial               rai              uri   \n",
        "109  good-partial               rai              uri   \n",
        "142  good-partial          data set          dataset   \n",
        "22          exact          metadata         metadata   \n",
        "87          exact        cataloging       cataloging   \n",
        "158         exact        adam smith      smith, adam   \n",
        "159         exact    charles darwin  darwin, charles   \n",
        "167         exact              cern             cern   \n",
        "\n",
        "                                               url  \\\n",
        "56   http://en.wikipedia.org/wiki/Geoffrey_Nunberg   \n",
        "106               http://en.wikipedia.org/wiki/RAI   \n",
        "109               http://en.wikipedia.org/wiki/RAI   \n",
        "142          http://en.wikipedia.org/wiki/Data_set   \n",
        "22           http://en.wikipedia.org/wiki/Metadata   \n",
        "87         http://en.wikipedia.org/wiki/Cataloging   \n",
        "158        http://en.wikipedia.org/wiki/Adam_Smith   \n",
        "159    http://en.wikipedia.org/wiki/Charles_Darwin   \n",
        "167              http://en.wikipedia.org/wiki/CERN   \n",
        "\n",
        "                                             wiki_text  \\\n",
        "56   Geoffrey Nunberg (born June, 1945) is an Ameri...   \n",
        "106  RAI &mdash; Radiotelevisione Italiana S.p.A. (...   \n",
        "109  RAI &mdash; Radiotelevisione Italiana S.p.A. (...   \n",
        "142  A dataset (or data set) is a collection of dat...   \n",
        "22   The term metadata refers to \"data about data\"....   \n",
        "87   Cataloging (or cataloguing) is the process of ...   \n",
        "158  Adam Smith (5 June 1723 OS \u2013 17 July 1790) was...   \n",
        "159  Charles Robert Darwin, FRS (12 February 1809 \u2013...   \n",
        "167  The European Organization for Nuclear Research...   \n",
        "\n",
        "                                            wiki_image  \n",
        "56   http://simple.wikipedia.org/w/index.php?title=...  \n",
        "106  http://simple.wikipedia.org/w/index.php?title=...  \n",
        "109  http://simple.wikipedia.org/w/index.php?title=...  \n",
        "142                                               None  \n",
        "22                                                None  \n",
        "87                                                None  \n",
        "158  http://simple.wikipedia.org/w/index.php?title=...  \n",
        "159  http://simple.wikipedia.org/w/index.php?title=...  \n",
        "167  http://simple.wikipedia.org/w/index.php?title=...  "
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Our result for Adam Smith (DataFram index 158)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import Image\n",
      "print matched_terms.ix[158]['wiki_text']\n",
      "Image(url=matched_terms.ix[158]['wiki_image']) \n",
      "# should limit size, but width=100, height=100 does not work \n",
      "# http://ipython.org/ipython-doc/dev/api/generated/IPython.core.display.html#IPython.core.display.Image"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adam Smith (5 June 1723 OS \u2013 17 July 1790) was a Scottish moral philosopher and a pioneer of political economy. One of the key figures of the Scottish Enlightenment, Adam Smith is best known for two classic works: The Theory of Moral Sentiments (1759), and An Inquiry into the Nature and Causes of the Wealth of Nations (1776). The latter, usually abbreviated as The Wealth of Nations, is considered his magnum opus and the first modern work of economics. Smith is cited as the \"father of modern economics\" and is still among the most influential thinkers in the field of economics today. In 2009, Smith was named among the \"Greatest Scots\" of all time, in a vote run by Scottish television channel STV.\n",
        "\n",
        "Smith studied social philosophy at the University of Glasgow and at Balliol College in the University of Oxford, where he was one of the first students to benefit from scholarships set up by his fellow Glaswegian John Snell. After graduating, he delivered a successful series of public lectures at the University of Edinburgh, leading him to collaborate with David Hume during the Scottish Enlightenment. Smith obtained a professorship at Glasgow teaching moral philosophy, and during this time he wrote and published The Theory of Moral Sentiments. In his later life, he took a tutoring position that allowed him to travel throughout Europe, where he met other intellectual leaders of his day. Smith then returned home and spent the next ten years writing The Wealth of Nations, publishing it in 1776. He died in 1790 at the age of 67.\n",
        "\n",
        "\n"
       ]
      },
      {
       "html": [
        "<img src=\"http://simple.wikipedia.org/w/index.php?title=Special:FilePath&file=AdamSmith.jpg\" />"
       ],
       "output_type": "pyout",
       "prompt_number": 134,
       "text": [
        "<IPython.core.display.Image at 0x10aa5af90>"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Other APIs - Flickr and News (Guardian and NYT)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Our project also incorporated Flickr and News APIs that run queries based on the terms matched from Wikipedia. These are potentially useful in various ways as shown in our prototype.</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Future Project Direction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>As a potentially useful tool for augmenting the reading experience with Open Data. Our group envisions two potential directions for our project or similar</p>\n",
      "<ol>\n",
      "    <li><b>Pre-publishing Processing</b> - where a non-fiction book in its raw form (such as DocBook XML) goes through a process where Open Data sources, such as a Wikipedia links, are inserted into the mark up. This step would take hours to run for an entire book. This would need improvement in the match quality algorithm and possibly human quality assurance of the inserted links.</li>\n",
      "    <li><b>Reading-time Ereader Extension</b> - where our tool (shown in the prototype) could be built as an extension to an open source ereader, such as the product under development by FuturePress. The extension would annotate any document a user is reading at reading-time with annotations linking entities to their Wikipedia pages. For this use case our matching algorithm would need a lot of improvement!</li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Appendix"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Note on Python String Encoding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>From <a href=\"http://farmdev.com/talks/unicode/\">http://farmdev.com/talks/unicode/</a></p>\n",
      "\n",
      "<p>When retrieving text from web resources and APIs and then storing the text, it is important to strictly convert strings to unicode. This will help to handle non-ascii characters before unicode only charaters start to create mystical bugs in the application. Areas where we ran into string encoding problems were attempting to store an DataFrame as a csv file and doing string comparisons (Can't compare a non-unicode string to a unicode string!). \n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Save/Open Data Frame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## write DataFrame\n",
      "import os\n",
      "path = os.getcwd() + '/wiki_urls_saved.csv'\n",
      "encoding = 'UTF-8'\n",
      "sep = ','\n",
      "matched_terms.to_csv(path,sep,encoding)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## read DataFrame\n",
      "new_df = pd.read_csv(path)\n",
      "new_df = new_df.drop(['Unnamed: 0'],axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "WikiUrlFetch Class (two working versions WikiUrlFetch and WikiUrlFetchNonDBPedia)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "\n",
      "from urllib2 import urlopen\n",
      "import urllib2\n",
      "import re\n",
      "import json\n",
      "from nltk import metrics\n",
      "from bs4 import BeautifulSoup, Tag\n",
      "\n",
      "class WikiUrlFetch():\n",
      "\n",
      "    def __init__(self):\n",
      "        self.cleaned_term = None\n",
      "\n",
      "    def fetch_wiki(self,term):\n",
      "        self.cleaned_term = self.clean_term(term)\n",
      "        self.results = self.get_wiki_url(self.cleaned_term)\n",
      "        return self.results\n",
      "\n",
      "    def clean_term(self,term):\n",
      "        return re.sub(r\"[^A-Za-z0-9 ]\",\"\",term.lower())\n",
      "        \n",
      "    def check_dbpedia(self, term):\n",
      "        api = 'http://lookup.dbpedia.org/api/search.asmx/KeywordSearch?MaxHits=5&QueryString='\n",
      "        #api = 'http://lookup.dbpedia.org/api/search.asmx/PrefixSearch?MaxHits=10&QueryString='\n",
      "       \n",
      "        #print term\n",
      "\n",
      "        try:\n",
      "            response = urlopen(api+term)\n",
      "        except:\n",
      "            return \"\"\n",
      "    \n",
      "        soup = BeautifulSoup(response.read())\n",
      "    \n",
      "        results = []\n",
      "        for result in soup.findAll('result'):\n",
      "            for child in result.children:\n",
      "                if isinstance(child,Tag):\n",
      "                    if child.name == 'label':\n",
      "                        current_label = child.string.lower()\n",
      "                    if child.name == 'uri':\n",
      "                        results.append({ 'term': current_label.encode('utf-8'), 'url': child.string.encode('utf-8') })\n",
      "        \n",
      "        return self.rank_dbpedia_results(results,term)\n",
      "    \n",
      "    def normalize(self,string):\n",
      "        strings = string.split(\" \")\n",
      "        strings.sort()\n",
      "        return \" \".join(strings)\n",
      "\n",
      "    def rank_dbpedia_results(self,results,term):\n",
      "        \"\"\"\n",
      "        logic:\n",
      "            if edit distance 0, exact match\n",
      "            if edit distance 1-4, good-partial match\n",
      "            if edit distance > 4, partial match (results unsorted)\n",
      "        \"\"\"\n",
      "\n",
      "        matches = []\n",
      "        for result in results:\n",
      "            matches.append([metrics.edit_distance(self.normalize(result['term']), self.normalize(term)), result])\n",
      "\n",
      "        matches.sort()\n",
      "        #print matches\n",
      "        if len(matches) == 0:\n",
      "            return [ { 'match': 'none', 'term': term.encode('utf-8') } ] \n",
      "        \n",
      "        elif matches[0][0] == 0:\n",
      "            new_results = [ matches[0][1] ]\n",
      "            new_results[0]['match'] = 'exact'\n",
      "            return new_results\n",
      "\n",
      "        elif matches[0][0] <= 8:\n",
      "            new_results = []\n",
      "            for match in matches:\n",
      "                if match[0] <= 3:\n",
      "                    result = match[1]\n",
      "                    result['match'] = 'good-partial'\n",
      "                    new_results.append(result)\n",
      "\n",
      "                \"\"\" # needs refinement to provide good matches\n",
      "                elif match[0] <= 5:\n",
      "                    words = self.normalize(result['term']).split(' ')\n",
      "                    if self.normalize(term) in words:\n",
      "                        result = match[1]\n",
      "                        result['match'] = 'good-partial'\n",
      "                        new_results.append(result)\n",
      "                \"\"\"\n",
      "            return new_results\n",
      "\n",
      "        else:\n",
      "            new_results = []\n",
      "            for result in results[0:2]:\n",
      "                result['match'] = 'partial'\n",
      "                new_results.append(result)\n",
      "            return new_results\n",
      "    \n",
      "    def wiki_url(self,url):\n",
      "\n",
      "        term = url[url.rfind('/'):]\n",
      "        entity_page = 'http://dbpedia.org/data/{}.json'.format(term)\n",
      "    \n",
      "        wiki_type = 'http://xmlns.com/foaf/0.1/primaryTopic'\n",
      "    \n",
      "        try:\n",
      "            response = urlopen(entity_page)\n",
      "        except:\n",
      "            return\n",
      "    \n",
      "        data = json.loads(response.read())\n",
      "        for key,value in data.items():\n",
      "            'http://xmlns.com/foaf/0.1/primaryTopic'\n",
      "            if 'http://xmlns.com/foaf/0.1/primaryTopic' in value:\n",
      "                #print key\n",
      "                return key.encode('utf-8')\n",
      "\n",
      "    def get_wiki_url(self, term):\n",
      "    \n",
      "        results = self.check_dbpedia(term)\n",
      "\n",
      "        for result in results:\n",
      "            if result['match'] != 'none' and result['match'] != 'partial':\n",
      "                wiki = self.wiki_url(result['url'])\n",
      "                result['wiki_url'] = wiki\n",
      "\n",
      "        return results\n",
      "\n",
      "class WikiUrlFetchNonDBPedia():\n",
      "\n",
      "    def __init__(self):\n",
      "        self.wiki_api =  'http://en.wikipedia.org/w/api.php?action=query&list=search&format=json&srsearch='\n",
      "        self.cleaned_term = None\n",
      "\n",
      "    def fetch_wiki(self,term):\n",
      "        self.cleaned_term = self.clean_term(term)\n",
      "        self.results = self.get_wiki_url(self.cleaned_term)\n",
      "        return self.results\n",
      "\n",
      "    def check_wikipedia_api(self,term):\n",
      "\n",
      "        url = self.wiki_api+re.sub(' ','_',term)\n",
      "        request = urllib2.Request(url)\n",
      "        request.add_header('User-Agent', 'Mozilla/5.0')\n",
      "\n",
      "        try:\n",
      "            response = urlopen(request)\n",
      "        except urllib2.HTTPError, e:\n",
      "            print e.code\n",
      "            return\n",
      "        except urllib2.URLError, e:\n",
      "            print e.reason\n",
      "            return\n",
      "\n",
      "        results = []\n",
      "        data = json.loads(response.read())\n",
      "        for key,value in data.items():\n",
      "            if 'search' in value:\n",
      "                if type(value['search']) == list:\n",
      "                    results = self.parse_wikipedia_results(value['search'])\n",
      "\n",
      "        if len(results) > 0:\n",
      "            return self.rank_dbpedia_results(results,term)\n",
      "        else:\n",
      "            return [ { 'match': 'none', 'term': term.encode('utf-8') } ]\n",
      "\n",
      "    def parse_wikipedia_results(self,results):\n",
      "        base_wikipedia_url = 'http://en.wikipedia.org/wiki/'\n",
      "        wiki_urls = []\n",
      "        for result in results:\n",
      "            if 'title' in result:\n",
      "                wiki_urls.append( \\\n",
      "                { 'url': (base_wikipedia_url + re.sub(' ','_',result['title'])).encode('UTF-8') , \\\n",
      "                'term': result['title'].encode('UTF-8') } )\n",
      "\n",
      "        return wiki_urls\n",
      "\n",
      "    def clean_term(self,term):\n",
      "        return re.sub(r\"[^A-Za-z0-9 ]\",\"\",term.lower())\n",
      "\n",
      "    def normalize(self,string):\n",
      "        strings = string.lower().split(\" \")\n",
      "        strings.sort()\n",
      "        return \" \".join(strings)\n",
      "\n",
      "    def rank_dbpedia_results(self,results,term):\n",
      "        \"\"\"\n",
      "        logic:\n",
      "            if edit distance 0, exact match\n",
      "            if edit distance 1-4, good-partial match\n",
      "            if edit distance > 4, partial match (results unsorted)\n",
      "        \"\"\"\n",
      "\n",
      "        matches = []\n",
      "        for result in results:\n",
      "            matches.append([metrics.edit_distance(self.normalize(result['term']), self.normalize(term)), result])\n",
      "\n",
      "        matches.sort()\n",
      "        print term,matches\n",
      "        print\n",
      "        if len(matches) == 0:\n",
      "            return [ { 'match': 'none', 'term': term.encode('utf-8') } ] \n",
      "\n",
      "        elif matches[0][0] == 0:\n",
      "            new_results = [ matches[0][1] ]\n",
      "            new_results[0]['match'] = 'exact'\n",
      "            return new_results\n",
      "\n",
      "        elif matches[0][0] <= 5:\n",
      "            new_results = []\n",
      "            for match in matches:\n",
      "                if match[0] <= 3:\n",
      "                    result = match[1]\n",
      "                    result['match'] = 'good-partial'\n",
      "                    new_results.append(result)\n",
      "\n",
      "                # needs refinement to provide good matches\n",
      "                elif match[0] <= 5:\n",
      "                    words = self.normalize(result['term']).split(' ')\n",
      "                    if self.normalize(term) in words:\n",
      "                        result = match[1]\n",
      "                        result['match'] = 'good-partial'\n",
      "                        new_results.append(result)\n",
      "\n",
      "            return new_results\n",
      "\n",
      "        else:\n",
      "            new_results = []\n",
      "            for result in results[0:2]:\n",
      "                result['match'] = 'partial'\n",
      "                new_results.append(result)\n",
      "            return new_results\n",
      "\n",
      "    def get_wiki_url(self, term):\n",
      "\n",
      "        results = self.check_wikipedia_api(term)\n",
      "\n",
      "        for result in results:\n",
      "            if result['match'] != 'none' and result['match'] != 'partial':\n",
      "                result['wiki_url'] = result['url']\n",
      "                \n",
      "\n",
      "        #print results\n",
      "        return results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Wiki2Plain Class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# source url:\n",
      "#!/usr/bin/env python\n",
      "\n",
      "import re\n",
      "import yaml\n",
      "import urllib\n",
      "import urllib2\n",
      "\n",
      "class WikipediaError(Exception):\n",
      "    pass\n",
      "\n",
      "class Wikipedia:\n",
      "    url_article = 'http://%s.wikipedia.org/w/index.php?action=raw&title=%s'\n",
      "    url_image = 'http://%s.wikipedia.org/w/index.php?title=Special:FilePath&file=%s'\n",
      "    url_search = 'http://%s.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&sroffset=%d&srlimit=%d&format=yaml'\n",
      "    \n",
      "    def __init__(self, lang):\n",
      "        self.lang = lang\n",
      "    \n",
      "    def __fetch(self, url):\n",
      "        request = urllib2.Request(url)\n",
      "        request.add_header('User-Agent', 'Mozilla/5.0')\n",
      "        \n",
      "        try:\n",
      "            result = urllib2.urlopen(request)\n",
      "        except urllib2.HTTPError, e:\n",
      "            raise WikipediaError(e.code)\n",
      "        except urllib2.URLError, e:\n",
      "            raise WikipediaError(e.reason)\n",
      "        \n",
      "        return result\n",
      "    \n",
      "    def article(self, url):\n",
      "        article = url[url.rfind('/')+1:]\n",
      "        url = self.url_article % (self.lang, urllib.quote_plus(article))\n",
      "        content = self.__fetch(url).read()\n",
      "        \n",
      "        if content.upper().startswith('#REDIRECT'):\n",
      "            match = re.match('(?i)#REDIRECT \\[\\[([^\\[\\]]+)\\]\\]', content)\n",
      "            \n",
      "            if not match == None:\n",
      "                return self.article(match.group(1))\n",
      "            \n",
      "            raise WikipediaError('Can\\'t found redirect article.')\n",
      "        \n",
      "        return content\n",
      "    \n",
      "    def image(self, image, thumb=None):\n",
      "        url = self.url_image % (self.lang, image)\n",
      "        result = self.__fetch(url)\n",
      "        content = result.read()\n",
      "        \n",
      "        if thumb:\n",
      "            url = result.geturl() + '/' + thumb + 'px-' + image\n",
      "            url = url.replace('/commons/', '/commons/thumb/')\n",
      "            url = url.replace('/' + self.lang + '/', '/' + self.lang + '/thumb/')\n",
      "            \n",
      "            return self.__fetch(url).read()\n",
      "        \n",
      "        return content\n",
      "    \n",
      "    def search(self, query, page=1, limit=10):\n",
      "        offset = (page - 1) * limit\n",
      "        url = self.url_search % (self.lang, urllib.quote_plus(query), offset, limit)\n",
      "        content = self.__fetch(url).read()\n",
      "        \n",
      "        parsed = yaml.load(content)\n",
      "        search = parsed['query']['search']\n",
      "        \n",
      "        results = []\n",
      "        \n",
      "        if search:\n",
      "            for article in search:\n",
      "                title = article['title'].strip()\n",
      "                \n",
      "                snippet = article['snippet']\n",
      "                snippet = re.sub(r'(?m)<.*?>', '', snippet)\n",
      "                snippet = re.sub(r'\\s+', ' ', snippet)\n",
      "                snippet = snippet.replace(' . ', '. ')\n",
      "                snippet = snippet.replace(' , ', ', ')\n",
      "                snippet = snippet.strip()\n",
      "                \n",
      "                wordcount = article['wordcount']\n",
      "                \n",
      "                results.append({\n",
      "                    'title' : title,\n",
      "                    'snippet' : snippet,\n",
      "                    'wordcount' : wordcount\n",
      "                })\n",
      "        \n",
      "        # yaml.dump(results, default_style='', default_flow_style=False,\n",
      "        #     allow_unicode=True)\n",
      "        return results\n",
      "\n",
      "class Wiki2Plain:\n",
      "#    url_article = 'http://%s.wikipedia.org/w/index.php?action=raw&title=%s'\n",
      "    url_image = 'http://%s.wikipedia.org/w/index.php?title=Special:FilePath&file=%s'\n",
      "#    url_search = 'http://%s.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&sroffset=%d&srlimit=%d&format=yaml'\n",
      "    \n",
      "    def __init__(self, url):\n",
      "        self.wiki = Wikipedia('en')\n",
      "        self.wiki_article = self.wiki.article(url)\n",
      "\n",
      "        self.text = self.wiki_article\n",
      "        self.text = self.unhtml(self.text)\n",
      "        self.text = self.unwiki(self.text)\n",
      "        self.text = self.punctuate(self.text)\n",
      "        self.text = self.get_summary(self.text)\n",
      "    \n",
      "    def __str__(self):\n",
      "        return self.text\n",
      "    \n",
      "    def unwiki(self, wiki):\n",
      "        \"\"\"\n",
      "        Remove wiki markup from the text.\n",
      "        \"\"\"\n",
      "        wiki = re.sub(r'(?i)\\{\\{IPA(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
      "        wiki = re.sub(r'(?i)\\{\\{Lang(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
      "        wiki = re.sub(r'\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
      "        wiki = re.sub(r'(?m)\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
      "        wiki = re.sub(r'(?m)\\{\\|[^\\{\\}]*?\\|\\}', '', wiki)\n",
      "        wiki = re.sub(r'(?i)\\[\\[Category:[^\\[\\]]*?\\]\\]', '', wiki)\n",
      "        wiki = re.sub(r'(?i)\\[\\[Image:[^\\[\\]]*?\\]\\]', '', wiki)\n",
      "        wiki = re.sub(r'(?i)\\[\\[File:[^\\[\\]]*?\\]\\]', '', wiki)\n",
      "        wiki = re.sub(r'\\[\\[[^\\[\\]]*?\\|([^\\[\\]]*?)\\]\\]', lambda m: m.group(1), wiki)\n",
      "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', lambda m: m.group(1), wiki)\n",
      "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', '', wiki)\n",
      "        wiki = re.sub(r'(?i)File:[^\\[\\]]*?', '', wiki)\n",
      "        wiki = re.sub(r'\\[[^\\[\\]]*? ([^\\[\\]]*?)\\]', lambda m: m.group(1), wiki)\n",
      "        wiki = re.sub(r\"''+\", '', wiki)\n",
      "        wiki = re.sub(r'(?m)^\\*$', '', wiki)\n",
      "        \n",
      "        return wiki\n",
      "    \n",
      "    def unhtml(self, html):\n",
      "        \"\"\"\n",
      "        Remove HTML from the text.\n",
      "        \"\"\"\n",
      "        html = re.sub(r'(?i)&nbsp;', ' ', html)\n",
      "        html = re.sub(r'(?i)<br[ \\\\]*?>', '\\n', html)\n",
      "        html = re.sub(r'(?m)<!--.*?--\\s*>', '', html)\n",
      "        html = re.sub(r'(?i)<ref[^>]*>[^>]*<\\/ ?ref>', '', html)\n",
      "        html = re.sub(r'(?m)<.*?>', '', html)\n",
      "        html = re.sub(r'(?i)&amp;', '&', html)\n",
      "        \n",
      "        return html\n",
      "    \n",
      "    def punctuate(self, text):\n",
      "        \"\"\"\n",
      "        Convert every text part into well-formed one-space\n",
      "        separate paragraph.\n",
      "        \"\"\"\n",
      "        text = re.sub(r'\\r\\n|\\n|\\r', '\\n', text)\n",
      "        text = re.sub(r'\\n\\n+', '\\n\\n', text)\n",
      "        \n",
      "        parts = text.split('\\n\\n')\n",
      "        partsParsed = []\n",
      "        \n",
      "        for part in parts:\n",
      "            part = part.strip()\n",
      "            \n",
      "            if len(part) == 0:\n",
      "                continue\n",
      "            \n",
      "            partsParsed.append(part)\n",
      "        \n",
      "        return '\\n\\n'.join(partsParsed)\n",
      "\n",
      "    def get_summary(self,text):\n",
      "        text = text[:text.find('==')]\n",
      "        return text\n",
      "\n",
      "    def image(self):\n",
      "        url_image = 'http://simple.wikipedia.org/w/index.php?title=Special:FilePath&file=' \n",
      "        \"\"\"\n",
      "        Retrieve the first image in the document.\n",
      "        \"\"\"\n",
      "        # match = re.search(r'(?i)\\|?\\s*(image|img|image_flag)\\s*=\\s*(<!--.*-->)?\\s*([^\\\\/:*?<>\"|%]+\\.[^\\\\/:*?<>\"|%]{3,4})', self.wiki)\n",
      "        match = re.search(r'= (\\b[\\w ]+\\b)+.(gif|jpg|jpeg|png|bmp)', self.wiki_article)\n",
      "        if match:\n",
      "            image_url = url_image + '%s.%s' % match.groups()\n",
      "            image_url = re.sub(' ', '_', image_url)\n",
      "            return image_url\n",
      "        \n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}