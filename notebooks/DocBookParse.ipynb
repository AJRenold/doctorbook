{
 "metadata": {
  "name": "DocBookParse"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Get Chapter1.xml and read it into BeautifulSoup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup, NavigableString, Tag\n",
      "import nltk\n",
      "import pandas as pd\n",
      "\n",
      "ORIGINAL_DRECTORY = '/Users/Desktop/docbookparser/'\n",
      "PATH = 'Chapter1.xml'\n",
      "soup = BeautifulSoup(open(PATH, 'rt').read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Check which tags are in the soup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book_tags = set([tag.name for tag in soup.findAll(True)])\n",
      "print(book_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['ulink', 'surname', 'figure', 'footnote', 'othername', 'primary', 'uri', 'hardware', 'citerefentry', 'manvolnum', 'phrase', 'sidebar', 'tertiary', 'xref', 'sect2', 'para', 'sect1', 'orgname', 'highlights', 'sect1info', 'abbrev', 'filename', 'application', 'emphasis', 'html', 'listitem', 'textobject', 'indexterm', 'chapterinfo', 'firstterm', 'body', 'keywordset', 'blockquote', 'attribution', 'firstname', 'quote', 'symbol', 'literal', 'sect3info', 'citetitle', 'link', 'foreignphrase', 'secondary', 'chapter', 'mediaobject', 'sidebarinfo', 'imagedata', 'keyword', 'glossterm', 'author', 'imageobject', 'personname', 'sect2info', 'refentrytitle', 'action', 'title', 'itemizedlist', 'sect3'])\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Get tags and terms and construct a Pandas DataFrame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "list_for_df = []\n",
      "tags = ['keyword','author','indexterm','orgname','personname','phrase']\n",
      "for tag in tags:\n",
      "    for i in soup.findAll(tag):\n",
      "        if i.string == None:\n",
      "            list_for_df.append({ 'term':\" \".join([ child for child in i.stripped_strings ]), \\\n",
      "                                'tag': tag })\n",
      "        else:\n",
      "            list_for_df.append({ 'term': re.sub('  +','',str(i.string)), 'tag': tag })\n",
      "        \n",
      "df = pd.DataFrame(list_for_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>tag</th>\n",
        "      <th>term</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td>       366</td>\n",
        "      <td>    366</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>unique</th>\n",
        "      <td>         6</td>\n",
        "      <td>    313</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>top</th>\n",
        "      <td> indexterm</td>\n",
        "      <td> Google</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>freq</th>\n",
        "      <td>       256</td>\n",
        "      <td>      8</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 87,
       "text": [
        "              tag    term\n",
        "count         366     366\n",
        "unique          6     313\n",
        "top     indexterm  Google\n",
        "freq          256       8"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Function to query DBPedia to find a term's wikipedia url (almost complete)\n",
      "Needs clean up and method to know when a term is not matched"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib2 import urlopen\n",
      "import json\n",
      "\n",
      "term = 'Barack Obama'\n",
      "\n",
      "def check_dbpedia(term):\n",
      "    api = 'http://lookup.dbpedia.org/api/search.asmx/KeywordSearch?MaxHits=10&QueryString='\n",
      "    #api = 'http://lookup.dbpedia.org/api/search.asmx/PrefixSearch?MaxHits=10&QueryString='\n",
      "    response = urlopen(api+term)\n",
      "    soup = BeautifulSoup(response.read())\n",
      "    \n",
      "    urls = []\n",
      "    for result in soup.findAll('result'):\n",
      "        for child in result.children:\n",
      "            if isinstance(child,Tag):\n",
      "                if child.name == 'label':\n",
      "                    current_label = child.string\n",
      "                if child.name == 'uri':\n",
      "                    urls.append({ 'label': current_label, 'url': child.string })\n",
      "    \n",
      "    #print urls\n",
      "    \n",
      "    ## exact match\n",
      "    found = \"\"\n",
      "    for url in urls:\n",
      "        if url['label'] == term:\n",
      "            url['match'] = 'exact'\n",
      "            found = url\n",
      "    \n",
      "    ## no exact match\n",
      "    if found == \"\":\n",
      "        found = urls[0:3]\n",
      "        for url in found:\n",
      "            url['match'] = 'partial'\n",
      "        \n",
      "    return found\n",
      "\n",
      "def wiki_url(url):\n",
      "\n",
      "    term = url[url.rfind('/'):]\n",
      "    \n",
      "    entity_page = 'http://dbpedia.org/data/{}.json'.format(term)\n",
      "    #print(entity_page)\n",
      "    \n",
      "    wiki_type = 'http://xmlns.com/foaf/0.1/primaryTopic'\n",
      "    \n",
      "    response = urlopen(entity_page)\n",
      "    data = json.loads(response.read())\n",
      "    for key,value in data.items():\n",
      "        'http://xmlns.com/foaf/0.1/primaryTopic'\n",
      "        #print(\"key\",value)\n",
      "        if 'http://xmlns.com/foaf/0.1/primaryTopic' in value:\n",
      "            return key\n",
      "\n",
      "def get_wiki_url(term):\n",
      "    \n",
      "    results = check_dbpedia(term)\n",
      "    #print results\n",
      "    \n",
      "    wikis = {}\n",
      "    \n",
      "    if type(results) == dict:\n",
      "        wiki = wiki_url(results['url'])\n",
      "        wikis['urls'] = [wiki]\n",
      "        wikis['match'] = 'exact'\n",
      "    \n",
      "    elif len(results) == 0:\n",
      "        wikis['match'] = 'none'\n",
      "    \n",
      "    else:\n",
      "        wikis['match'] = 'partial'\n",
      "        wikis['urls'] = []\n",
      "        for result in results:\n",
      "            wiki = wiki_url(result['url'])\n",
      "            wikis['urls'].append(wiki)\n",
      "            \n",
      "    return wikis\n",
      "    \n",
      "urls = get_wiki_url('Super Bowl')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print urls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'match': 'exact', 'urls': [u'http://en.wikipedia.org/wiki/Super_Bowl']}\n"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Use .apply(check_dpedia) to run function across all terms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['wiki_url'] = df['term'].apply(check_dbpedia)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 191
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}